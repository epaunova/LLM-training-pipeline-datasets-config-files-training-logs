# ğŸ§  LLM Prompt Engineering Library

A curated repository of high-quality prompts, templates, and workflows for training, testing, and evaluating Large Language Models (LLMs).

Built by Eva Paunova, this repo serves as a living collection of:
- ğŸ’¡ Prompt design strategies
- ğŸ” Evaluation rubrics and examples
- ğŸ“Š Structured prompt datasets
- ğŸ”§ Scripts for formatting and fine-tuning
- ğŸ“˜ Templates for real-world applications (product management, education, classification, reasoning, etc.)

---

## ğŸ“ Repository Structure

```
llm-prompt-engineering/
â”‚
â”œâ”€â”€ prompts/                # Prompt libraries by category and use case
â”‚   â”œâ”€â”€ product_management_prompts.csv
â”‚   â”œâ”€â”€ prompt_engineering_basic.csv
â”‚   â”œâ”€â”€ prompt_engineering_advanced.csv
â”‚
â”œâ”€â”€ templates/              # Meta-prompt scaffolds, system instructions
â”‚   â””â”€â”€ evaluation_rubric.md
â”‚
â”œâ”€â”€ notebooks/              # Testbed notebooks for prompt output analysis
â”‚   â””â”€â”€ prompt_testing.ipynb
â”‚
â”œâ”€â”€ datasets/               # Training/evaluation datasets (optional)
â”‚
â”œâ”€â”€ scripts/                # Automation scripts for prompt formatting, export
â”‚   â””â”€â”€ convert_to_finetune_json.py
â”‚
â””â”€â”€ README.md               # This file
```

---

## ğŸ” Prompt Libraries

### âœ… Product Management
Prompts to simulate daily workflows of a PM:
- User research synthesis
- Roadmap prioritization (RICE, Kano, OKRs)
- PRD writing, backlog grooming, sprint planning
- Stakeholder communication and go-to-market strategy

### ğŸ›  Prompt Engineering (Basic + Advanced)
- Few-shot, chain-of-thought, zero-shot, multi-turn design
- Meta-prompts and evaluation scaffolds
- Token efficiency optimization
- Bias reduction, multilingual prompting, rubric design

---

## ğŸ¤– Ideal For

- LLM trainers, researchers, and applied ML teams
- Prompt engineers working on RAG, agents, or fine-tuning
- Technical PMs and AI product builders
- OpenAI/Anthropic/Claude/LLaMA model testers

---

## ğŸš€ Getting Started

1. Clone this repo
2. Open any prompt CSV in `/prompts`
3. Load it into your LLM playground or use the scripts to convert to OpenAI fine-tune format
4. Use `/notebooks` to run evaluations, score responses, or visualize token costs

---

## ğŸ“œ License

MIT License

---

## âœ¨ Contributions Welcome

Want to add your own prompt library or prompt evaluation tool? See [`CONTRIBUTING.md`](./CONTRIBUTING.md)

---
